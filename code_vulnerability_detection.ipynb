{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN/sDSK9Sntzapgo/txNoDl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emanuelebrizzi/bootcamp/blob/main/code_vulnerability_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCJT8cLSu_la",
        "outputId": "22b1d073-1407-46d4-881e-b35ab37c2ba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXYDvCvi5XZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b867a5a-b8ed-4397-deea-2667dc5d4c8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import files\n",
        "#import zipfile\n",
        "#import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data upload**\n"
      ],
      "metadata": {
        "id": "vwxvuUJnjbIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Directory path\n",
        "drive_path = \"/content/drive/MyDrive/bootcamp_file\"\n",
        "\n",
        "libpng_path = os.path.join(drive_path, \"LibPNG\")\n",
        "\n",
        "libpng_vuln_path = os.path.join(libpng_path, \"Vulnerable_functions\")\n",
        "libpng_non_vuln_path = os.path.join(libpng_path, \"Non_vulnerable_functions\")\n",
        "\n",
        "vlc_path = os.path.join(drive_path, \"VLC\")\n",
        "\n",
        "vlc_vuln_path = os.path.join(vlc_path, \"Vulnerable_functions\")\n",
        "vlc_non_vuln_path = os.path.join(vlc_path, \"Non_vulnerable_functions\")\n",
        "\n",
        "pidgin_path = os.path.join(drive_path, \"Pidgin\")\n",
        "\n",
        "pidgin_vuln_path = os.path.join(pidgin_path, \"Vulnerable_functions\")\n",
        "pidgin_non_vuln_path = os.path.join(pidgin_path, \"Non_vulnerable_functions\")"
      ],
      "metadata": {
        "id": "9bSkiYW8bdvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_files_from_folder(folder, label):\n",
        "    files = glob.glob(os.path.join(folder, \"*.c\"))\n",
        "    data = []\n",
        "    for file in files:\n",
        "        with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            data.append((f.read(), label, os.path.basename(file)))\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "LuhDeKSyd9Gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Replace Function names and Variable names**"
      ],
      "metadata": {
        "id": "GrfMYRimjn3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import string\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "PfJOtGKtW-q3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes comments from C code\n",
        "def remove_comments(code):\n",
        "    code = re.sub(r'//.*', '', code)\n",
        "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
        "    return code\n",
        "\n",
        "# Extracts variable and function names from C code\n",
        "def extract_identifiers(code):\n",
        "    pattern = re.compile(r'\\b([a-zA-Z_][a-zA-Z0-9_]*)\\b')\n",
        "    keywords = set(['int', 'char', 'float', 'double', 'return', 'if', 'else', 'while', 'for', 'do', 'switch', 'case', 'void'])\n",
        "    identifiers = set(pattern.findall(code)) - keywords\n",
        "    return identifiers\n",
        "\n",
        "# Classifies identifiers as variables or functions\n",
        "def categorize_identifiers(identifiers):\n",
        "    var_counter, fun_counter = 1, 1\n",
        "    replacements = {}\n",
        "    for identifier in identifiers:\n",
        "        if re.search(r'\\b[A-Za-z_][A-Za-z0-9_]*\\s*\\(', identifier):\n",
        "            replacements[identifier] = f'fun_{fun_counter}'\n",
        "            fun_counter += 1\n",
        "        else:\n",
        "            replacements[identifier] = f'var_{var_counter}'\n",
        "            var_counter += 1\n",
        "    return replacements\n",
        "\n",
        "# Replaces variable and function names with new names\n",
        "def replace_identifiers(code, replacements):\n",
        "\n",
        "    for old, new in replacements.items():\n",
        "        code = re.sub(r'\\b' + re.escape(old) + r'\\b', new, code)\n",
        "    return code\n",
        "\n",
        "# Reads a C file, replaces names, and saves the new file\n",
        "def process_c_file(filepath, new_filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        code = file.read()\n",
        "\n",
        "    code_no_comments = remove_comments(code)\n",
        "    identifiers = extract_identifiers(code_no_comments)\n",
        "    replacements = categorize_identifiers(identifiers)\n",
        "    new_code = replace_identifiers(code_no_comments, replacements)\n",
        "    with open(new_filepath, 'w', encoding='utf-8') as file:\n",
        "        file.write(new_code)\n",
        "\n",
        "# Processes all C files in a directory\n",
        "def process_directory(directory, new_directory_path):\n",
        "    for filepath in Path(directory).glob(\"*.c\"):\n",
        "        new_filepath = os.path.join(new_directory_path, os.path.basename(filepath))\n",
        "        process_c_file(filepath, new_filepath)"
      ],
      "metadata": {
        "id": "zqT_F2_Eoh7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleanning code:"
      ],
      "metadata": {
        "id": "0X-adtx-kMaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Creation modified directory\n",
        "modified_path = os.path.join(drive_path, \"modified_files\")\n",
        "modified_vuln_path = os.path.join(modified_path, \"Vulnerable_functions\")\n",
        "modified_non_vuln_path = os.path.join(modified_path, \"Non_vulnerable_functions\")\n",
        "\n",
        "if os.path.exists(modified_vuln_path):\n",
        "  shutil.rmtree(modified_vuln_path)\n",
        "os.makedirs(modified_vuln_path, exist_ok=True)\n",
        "\n",
        "if os.path.exists(modified_non_vuln_path):\n",
        "  shutil.rmtree(modified_non_vuln_path)\n",
        "os.makedirs(modified_non_vuln_path, exist_ok=True)\n",
        "\n",
        "\n",
        "if os.path.exists(modified_vuln_path):\n",
        "  shutil.rmtree(modified_vuln_path)\n",
        "os.makedirs(modified_vuln_path, exist_ok=True)\n",
        "\n",
        "if os.path.exists(modified_non_vuln_path):\n",
        "  shutil.rmtree(modified_non_vuln_path)\n",
        "os.makedirs(modified_non_vuln_path, exist_ok=True)\n",
        "\n",
        "process_directory(libpng_vuln_path, modified_vuln_path)\n",
        "process_directory(libpng_non_vuln_path, modified_non_vuln_path)\n"
      ],
      "metadata": {
        "id": "LyBFy5YFsLh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BLSTM**"
      ],
      "metadata": {
        "id": "5HVkeHkAiXiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "# Load and label data\n",
        "def load_data(directory, label):\n",
        "    data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        with open(os.path.join(directory, filename), 'r') as file:\n",
        "            data.append((file.read(), label))\n",
        "    return data\n",
        "\n",
        "vulnerable_files = load_data(modified_vuln_path, label=1)\n",
        "non_vulnerable_files = load_data(modified_non_vuln_path, label=0)\n",
        "\n",
        "# Combine and shuffle dataset\n",
        "dataset = vulnerable_files + non_vulnerable_files\n",
        "np.random.shuffle(dataset)\n",
        "\n",
        "texts, labels = zip(*dataset)\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Pad sequences\n",
        "max_length = 500  # Adjust based on dataset\n",
        "data = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Split dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define BLSTM model\n",
        "embedding_dim = 128\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model on training data\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Evaluate model on test data\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}\")\n",
        "print(f\"Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Predict on test data\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScxPHUSJiZco",
        "outputId": "d2a2b99e-80db-408a-b1b5-1829e585315c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 86ms/step - accuracy: 0.7257 - loss: 0.5365 - val_accuracy: 0.9600 - val_loss: 0.1510\n",
            "Epoch 2/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9280 - loss: 0.3050 - val_accuracy: 0.9600 - val_loss: 0.1577\n",
            "Epoch 3/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.9324 - loss: 0.2246 - val_accuracy: 0.9600 - val_loss: 0.1442\n",
            "Epoch 4/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9076 - loss: 0.2855 - val_accuracy: 0.9600 - val_loss: 0.1351\n",
            "Epoch 5/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9272 - loss: 0.2121 - val_accuracy: 0.9600 - val_loss: 0.1245\n",
            "Epoch 6/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9341 - loss: 0.1999 - val_accuracy: 0.9600 - val_loss: 0.1074\n",
            "Epoch 7/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9329 - loss: 0.1348 - val_accuracy: 0.9800 - val_loss: 0.1165\n",
            "Epoch 8/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9525 - loss: 0.1292 - val_accuracy: 0.9600 - val_loss: 0.0987\n",
            "Epoch 9/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9432 - loss: 0.1039 - val_accuracy: 0.9800 - val_loss: 0.0873\n",
            "Epoch 10/10\n",
            "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9772 - loss: 0.0748 - val_accuracy: 0.9600 - val_loss: 0.0946\n",
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.9022 - loss: 0.2747\n",
            "Test Loss: 0.30269742012023926\n",
            "Test Accuracy: 0.8960000276565552\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 25 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79035c44fd80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[1m1/4\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 194ms/step"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 28 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79035c44fd80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9Foi22mzS-1",
        "outputId": "a0d93b50-c08b-482d-a118-b11ad502a211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "622"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "itp5RVki7fb3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}