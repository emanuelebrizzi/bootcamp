{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPWwdjQbW5ibCuMRF93hsZG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emanuelebrizzi/bootcamp/blob/main/code_vulnerability_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCJT8cLSu_la",
        "outputId": "4171da57-8262-4c3e-e8a5-b979a6affda3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "TXYDvCvi5XZq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43b9bce6-a941-4869-8719-733a7ecf7735"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#from google.colab import files\n",
        "#import zipfile\n",
        "#import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dataset upload**\n"
      ],
      "metadata": {
        "id": "vwxvuUJnjbIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "\n",
        "# Directory path\n",
        "drive_path = \"/content/drive/MyDrive/bootcamp_file\"\n",
        "\n",
        "libpng_path = os.path.join(drive_path, \"LibPNG\")\n",
        "\n",
        "vuln_path = os.path.join(libpng_path, \"Vulnerable_functions\")\n",
        "non_vuln_path = os.path.join(libpng_path, \"Non_vulnerable_functions\")"
      ],
      "metadata": {
        "id": "9bSkiYW8bdvj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def load_files_from_folder(folder, label):\n",
        "    files = glob.glob(os.path.join(folder, \"*.c\"))\n",
        "    data = []\n",
        "    for file in files:\n",
        "        with open(file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "            data.append((f.read(), label, os.path.basename(file)))\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "LuhDeKSyd9Gf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Replace Function names and Variable names**"
      ],
      "metadata": {
        "id": "GrfMYRimjn3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import random\n",
        "import string\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "PfJOtGKtW-q3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removes comments from C code\n",
        "def remove_comments(code):\n",
        "    code = re.sub(r'//.*', '', code)\n",
        "    code = re.sub(r'/\\*.*?\\*/', '', code, flags=re.DOTALL)\n",
        "    return code\n",
        "\n",
        "# Extracts variable and function names from C code\n",
        "def extract_identifiers(code):\n",
        "    pattern = re.compile(r'\\b([a-zA-Z_][a-zA-Z0-9_]*)\\b')\n",
        "    keywords = set(['int', 'char', 'float', 'double', 'return', 'if', 'else', 'while', 'for', 'do', 'switch', 'case', 'void'])\n",
        "    identifiers = set(pattern.findall(code)) - keywords\n",
        "    return identifiers\n",
        "\n",
        "# Classifies identifiers as variables or functions\n",
        "def categorize_identifiers(identifiers):\n",
        "    var_counter, fun_counter = 1, 1\n",
        "    replacements = {}\n",
        "    for identifier in identifiers:\n",
        "        if re.search(r'\\b[A-Za-z_][A-Za-z0-9_]*\\s*\\(', identifier):\n",
        "            replacements[identifier] = f'fun_{fun_counter}'\n",
        "            fun_counter += 1\n",
        "        else:\n",
        "            replacements[identifier] = f'var_{var_counter}'\n",
        "            var_counter += 1\n",
        "    return replacements\n",
        "\n",
        "# Replaces variable and function names with new names\n",
        "def replace_identifiers(code, replacements):\n",
        "\n",
        "    for old, new in replacements.items():\n",
        "        code = re.sub(r'\\b' + re.escape(old) + r'\\b', new, code)\n",
        "    return code\n",
        "\n",
        "# Reads a C file, replaces names, and saves the new file\n",
        "def process_c_file(filepath, new_filepath):\n",
        "    with open(filepath, 'r', encoding='utf-8') as file:\n",
        "        code = file.read()\n",
        "\n",
        "    code_no_comments = remove_comments(code)\n",
        "    identifiers = extract_identifiers(code_no_comments)\n",
        "    replacements = categorize_identifiers(identifiers)\n",
        "    new_code = replace_identifiers(code_no_comments, replacements)\n",
        "    with open(new_filepath, 'w', encoding='utf-8') as file:\n",
        "        file.write(new_code)\n",
        "\n",
        "# Processes all C files in a directory\n",
        "def process_directory(directory, new_directory_path):\n",
        "    for filepath in Path(directory).glob(\"*.c\"):\n",
        "        new_filepath = os.path.join(new_directory_path, os.path.basename(filepath))\n",
        "        process_c_file(filepath, new_filepath)"
      ],
      "metadata": {
        "id": "zqT_F2_Eoh7P"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleanning code:"
      ],
      "metadata": {
        "id": "0X-adtx-kMaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "modified_vuln_path = os.path.join(vuln_path, \"modified_files\")\n",
        "\n",
        "if os.path.exists(modified_vuln_path):\n",
        "  shutil.rmtree(modified_vuln_path)\n",
        "os.makedirs(modified_vuln_path, exist_ok=True)\n",
        "process_directory(vuln_path, modified_vuln_path)\n",
        "\n",
        "\n",
        "modified_non_vuln_path = os.path.join(non_vuln_path, \"modified_files\")\n",
        "\n",
        "if os.path.exists(modified_non_vuln_path):\n",
        "  shutil.rmtree(modified_non_vuln_path)\n",
        "os.makedirs(modified_non_vuln_path, exist_ok=True)\n",
        "process_directory(non_vuln_path, modified_non_vuln_path)"
      ],
      "metadata": {
        "id": "LyBFy5YFsLh9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BLSTM**"
      ],
      "metadata": {
        "id": "5HVkeHkAiXiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "\n",
        "# Load and label data\n",
        "def load_data(directory, label):\n",
        "    data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        with open(os.path.join(directory, filename), 'r') as file:\n",
        "            data.append((file.read(), label))\n",
        "    return data\n",
        "\n",
        "vulnerable_files = load_data(modified_vuln_path, label=1)\n",
        "non_vulnerable_files = load_data(modified_non_vuln_path, label=0)\n",
        "\n",
        "# Combine and shuffle dataset\n",
        "dataset = vulnerable_files + non_vulnerable_files\n",
        "np.random.shuffle(dataset)\n",
        "\n",
        "texts, labels = zip(*dataset)\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Pad sequences\n",
        "max_length = 500  # Adjust based on dataset\n",
        "data = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Define BLSTM model\n",
        "embedding_dim = 128\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word_index) + 1, output_dim=embedding_dim, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=False)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(data, labels, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict on new data\n",
        "predictions = model.predict(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScxPHUSJiZco",
        "outputId": "9684a415-39e2-4665-eba5-cef515097361"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.9155 - loss: 0.4958 - val_accuracy: 0.9360 - val_loss: 0.2718\n",
            "Epoch 2/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 40ms/step - accuracy: 0.9234 - loss: 0.2672 - val_accuracy: 0.9360 - val_loss: 0.2356\n",
            "Epoch 3/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.9345 - loss: 0.2171 - val_accuracy: 0.9360 - val_loss: 0.2228\n",
            "Epoch 4/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 74ms/step - accuracy: 0.9320 - loss: 0.2244 - val_accuracy: 0.9360 - val_loss: 0.2121\n",
            "Epoch 5/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 56ms/step - accuracy: 0.9315 - loss: 0.1987 - val_accuracy: 0.9360 - val_loss: 0.1998\n",
            "Epoch 6/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9159 - loss: 0.2046 - val_accuracy: 0.9360 - val_loss: 0.1818\n",
            "Epoch 7/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9090 - loss: 0.1808 - val_accuracy: 0.9280 - val_loss: 0.1757\n",
            "Epoch 8/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9569 - loss: 0.1236 - val_accuracy: 0.9360 - val_loss: 0.1724\n",
            "Epoch 9/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.9804 - loss: 0.0880 - val_accuracy: 0.9520 - val_loss: 0.1772\n",
            "Epoch 10/10\n",
            "\u001b[1m16/16\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9819 - loss: 0.0809 - val_accuracy: 0.9280 - val_loss: 0.2647\n",
            "\u001b[1m20/20\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step\n"
          ]
        }
      ]
    }
  ]
}